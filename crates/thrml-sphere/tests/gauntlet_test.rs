//! Self-Cannibalization Gauntlet Test
//!
//! Tests the full pipeline: BLT SafeTensors ‚Üí ROOTS Index ‚Üí MultiConeNavigator
//!
//! To run this test:
//! 1. First run the blt-burn ingestion to create test data:
//!    ```bash
//!    cd external/blt-burn && cargo run --release --bin ingest -- \
//!      --file /tmp/burn-corpus/crates/burn-tensor/src/tests/module/conv2d.rs \
//!      --model-path ~/.cache/huggingface/hub/models--facebook--blt-entropy/snapshots/*/model.safetensors \
//!      --output-dir /tmp/gauntlet_output \
//!      --sphere --sphere-scale medium --entropy-weighted
//!    ```
//! 2. Run this test:
//!    ```bash
//!    cargo test --package thrml-sphere --test gauntlet_test -- --nocapture
//!    ```

use std::path::Path;
use std::time::Instant;

use burn::tensor::Tensor;
use thrml_core::backend::{init_gpu_device, WgpuBackend};
// HardwareTier used for BudgetConfig::for_tier if needed
#[allow(unused_imports)]
use thrml_core::compute::HardwareTier;
use thrml_samplers::RngKey;
use thrml_sphere::{
    load_blt_safetensors, BudgetConfig, MultiConeNavigator, NavigationWeights, RootsConfig,
    ScaleProfile, SphereConfig,
};

// Feature-gated hypergraph loading
#[cfg(feature = "hypergraph")]
use thrml_sphere::{
    hypergraph_stats, load_blt_with_hypergraph, HypergraphLoadConfig, NavigatorEBM,
};

/// Path to gauntlet test data (generated by blt-burn ingestion)
/// Priority: ai drive (fastest) > CrucialX6 > legacy fineweb > legacy conv2d
const GAUNTLET_AI_CORPUS: &str = "/Volumes/ai/sphere_results/corpus.safetensors";
const GAUNTLET_CORPUS: &str = "/Volumes/CrucialX6/sphere_results/corpus.safetensors";
const GAUNTLET_FINEWEB: &str =
    "/tmp/gauntlet_fineweb/item_<urn:uuid:c337bcd8-6aa1-4f2d-8c48-b916442ebbee>.safetensors";
const GAUNTLET_CONV2D: &str = "/tmp/gauntlet_output/conv2d.safetensors";

/// Maximum patches for ROOTS construction in tests.
/// ROOTS has O(n¬≤) pairwise similarity, so we limit to keep tests fast:
/// - 200 patches ‚Üí 40,000 comparisons (~10s)
/// - 927 patches ‚Üí 859,329 comparisons (~8+ min)
const MAX_PATCHES_FOR_ROOTS: usize = 200;

fn get_gauntlet_path() -> &'static str {
    if Path::new(GAUNTLET_AI_CORPUS).exists() {
        GAUNTLET_AI_CORPUS
    } else if Path::new(GAUNTLET_CORPUS).exists() {
        GAUNTLET_CORPUS
    } else if Path::new(GAUNTLET_FINEWEB).exists() {
        GAUNTLET_FINEWEB
    } else {
        GAUNTLET_CONV2D
    }
}

/// Skip test if gauntlet data doesn't exist
fn skip_if_no_gauntlet_data() -> bool {
    let path = get_gauntlet_path();
    if !Path::new(path).exists() {
        eprintln!("‚ö†Ô∏è  Skipping gauntlet test: no test data found");
        eprintln!("   Expected: {} or {}", GAUNTLET_FINEWEB, GAUNTLET_CONV2D);
        eprintln!("   Run the blt-burn ingestion first (see test docstring)");
        return true;
    }
    false
}

#[test]
fn test_gauntlet_load_blt_safetensors() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let path = Path::new(get_gauntlet_path());

    println!("=== Gauntlet Test: Loading BLT SafeTensors ===");
    println!("Using: {}", path.display());
    let start = Instant::now();

    // Load BLT v3 format with embeddings and raw bytes
    let result = load_blt_safetensors(path, SphereConfig::default(), &device);

    match result {
        Ok((sphere_ebm, patch_bytes)) => {
            let elapsed = start.elapsed();
            println!("‚úÖ Loaded in {:?}", elapsed);

            // Verify structure
            let n_patches = sphere_ebm.n_points();
            let embed_dim = sphere_ebm.embedding_dim();

            println!("   Patches: {}", n_patches);
            println!("   Embedding dim: {}", embed_dim);
            println!("   Patch bytes: {} vectors", patch_bytes.len());

            assert!(n_patches > 0, "Should have at least one patch");
            assert!(
                embed_dim == 768 || embed_dim == 2048,
                "BLT embeddings should be 768-dim (entropy) or 2048-dim (BLT-1B), got {}",
                embed_dim
            );
            assert_eq!(
                patch_bytes.len(),
                n_patches,
                "Should have bytes for each patch"
            );

            // Show sample patches
            println!("\n   Sample patches:");
            for (i, bytes) in patch_bytes.iter().take(3).enumerate() {
                let preview = String::from_utf8_lossy(&bytes[..bytes.len().min(50)]);
                println!("   [{:3}] {} bytes: {:?}...", i, bytes.len(), preview);
            }
        }
        Err(e) => {
            panic!("Failed to load BLT SafeTensors: {}", e);
        }
    }
}

#[test]
fn test_gauntlet_roots_index_construction() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let path = Path::new(get_gauntlet_path());

    println!("=== Gauntlet Test: ROOTS Index Construction ===");

    // Load data
    let (sphere_ebm, patch_bytes) =
        load_blt_safetensors(path, SphereConfig::default(), &device).expect("Failed to load");

    let n_patches = sphere_ebm.n_points();
    println!("Loaded {} patches", n_patches);

    // ROOTS has O(n¬≤) pairwise similarity - skip if corpus too large
    if n_patches > MAX_PATCHES_FOR_ROOTS {
        println!(
            "‚ö†Ô∏è  Skipping ROOTS construction: {} patches > {} limit (O(n¬≤) would take too long)",
            n_patches, MAX_PATCHES_FOR_ROOTS
        );
        println!("   Set MAX_PATCHES_FOR_ROOTS higher or use smaller corpus for this test");
        return;
    }

    // Build ROOTS with substring coupling for code
    let roots_config = RootsConfig::default()
        .with_partitions(32) // Use 32 partitions for this test size
        .with_default_substring_coupling(); // 70% embedding, 30% byte overlap

    println!("Building ROOTS index with substring coupling...");
    let start = Instant::now();

    let navigator = MultiConeNavigator::from_sphere_ebm_with_bytes(
        &sphere_ebm,
        &patch_bytes,
        roots_config,
        BudgetConfig::dev(),
        RngKey::new(42),
        &device,
    );

    let elapsed = start.elapsed();
    println!("‚úÖ ROOTS built in {:?}", elapsed);

    // Verify ROOTS structure
    let stats = navigator.roots.stats();
    println!("\nROOTS Index Stats:");
    println!("{}", stats);

    assert!(stats.n_partitions >= 1, "Should have at least 1 partition");
    assert!(
        stats.memory_bytes < 10 * 1024 * 1024,
        "Index should be < 10MB"
    );

    // Calculate compression ratio
    let embed_dim = sphere_ebm.embedding_dim();
    let raw_size = n_patches * embed_dim * 4; // N * D * sizeof(f32)
    let compression_ratio = raw_size as f64 / stats.memory_bytes as f64;
    println!(
        "\nCompression: {} bytes raw ‚Üí {} bytes index ({:.1}:1 ratio)",
        raw_size, stats.memory_bytes, compression_ratio
    );

    // Note: Small datasets (< 1000 points) don't compress well because
    // ROOTS overhead (centroids, classifier) dominates. At scale (100K+ points),
    // compression ratios of 100:1+ are typical.
    assert!(
        compression_ratio > 1.0,
        "Index should be smaller than raw embeddings"
    );
}

#[test]
fn test_gauntlet_multi_cone_navigation() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let path = Path::new(get_gauntlet_path());

    println!("=== Gauntlet Test: Multi-Cone Navigation ===");

    // Load data
    let (sphere_ebm, patch_bytes) =
        load_blt_safetensors(path, SphereConfig::from(ScaleProfile::Dev), &device)
            .expect("Failed to load");

    let n_patches = sphere_ebm.n_points();
    println!("Loaded {} patches", n_patches);

    // ROOTS has O(n¬≤) pairwise similarity - skip if corpus too large
    if n_patches > MAX_PATCHES_FOR_ROOTS {
        println!(
            "‚ö†Ô∏è  Skipping multi-cone navigation: {} patches > {} limit (ROOTS O(n¬≤) too slow)",
            n_patches, MAX_PATCHES_FOR_ROOTS
        );
        return;
    }

    // Build navigator
    let roots_config = RootsConfig::dev().with_partitions(16).with_threshold(0.1); // Lower threshold to ensure cone spawning

    let budget_config = BudgetConfig::dev();

    let mut navigator = MultiConeNavigator::from_sphere_ebm_with_bytes(
        &sphere_ebm,
        &patch_bytes,
        roots_config,
        budget_config,
        RngKey::new(42),
        &device,
    );

    // Create a query embedding (use first patch embedding as query)
    let embeddings = sphere_ebm.embeddings.clone();
    let embed_dim = sphere_ebm.embedding_dim();
    let query: Tensor<WgpuBackend, 1> = embeddings.slice([0..1, 0..embed_dim]).reshape([embed_dim]);

    println!("\nRunning multi-cone navigation...");
    let start = Instant::now();

    let result = navigator.navigate_multi_cone(query, 50.0, 10, RngKey::new(123), &device);

    let elapsed = start.elapsed();
    println!("‚úÖ Navigation completed in {:?}", elapsed);

    // Report results
    println!("\nNavigation Results:");
    println!("  Cones spawned: {}", result.n_cones());
    println!("  Targets found: {}", result.n_targets());
    println!("  Budget used: {} bytes", result.budget_used);

    if !result.is_empty() {
        println!("\nTop results:");
        for (i, (idx, energy)) in result
            .target_indices
            .iter()
            .zip(result.target_energies.iter())
            .take(5)
            .enumerate()
        {
            let patch = &patch_bytes[*idx];
            let preview = String::from_utf8_lossy(&patch[..patch.len().min(60)]);
            println!(
                "  {}. [{}] energy={:.4}: {:?}...",
                i + 1,
                idx,
                energy,
                preview
            );
        }
    }

    // Verify we got results
    assert!(
        !result.is_empty() || result.n_cones() == 0,
        "If cones spawned, should have results"
    );

    // Navigation stats
    let stats = navigator.last_navigation_stats();
    println!("\nNavigation Stats: {}", stats);
}

#[test]
fn test_gauntlet_code_query_relevance() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let path = Path::new(get_gauntlet_path());

    println!("=== Gauntlet Test: Code Query Relevance ===");

    // Load data
    let (sphere_ebm, patch_bytes) =
        load_blt_safetensors(path, SphereConfig::from(ScaleProfile::Dev), &device)
            .expect("Failed to load");

    let n_patches = sphere_ebm.n_points();

    // ROOTS has O(n¬≤) pairwise similarity - skip if corpus too large
    if n_patches > MAX_PATCHES_FOR_ROOTS {
        println!(
            "‚ö†Ô∏è  Skipping code query relevance: {} patches > {} limit (ROOTS O(n¬≤) too slow)",
            n_patches, MAX_PATCHES_FOR_ROOTS
        );
        return;
    }

    // Build navigator with semantic-focused weights
    let roots_config = RootsConfig::dev()
        .with_partitions(16)
        .with_default_substring_coupling();

    let mut navigator = MultiConeNavigator::from_sphere_ebm_with_bytes(
        &sphere_ebm,
        &patch_bytes,
        roots_config,
        BudgetConfig::dev(),
        RngKey::new(42),
        &device,
    )
    .with_weights(
        NavigationWeights::default()
            .with_semantic(1.0)
            .with_radial(0.2),
    );

    // Find patches containing "conv2d" by scanning
    let conv2d_patches: Vec<(usize, &[u8])> = patch_bytes
        .iter()
        .enumerate()
        .filter(|(_, bytes)| {
            let text = String::from_utf8_lossy(bytes).to_lowercase();
            text.contains("conv2d") || text.contains("convolution")
        })
        .map(|(i, b)| (i, b.as_slice()))
        .collect();

    println!(
        "Found {} patches containing 'conv2d' or 'convolution'",
        conv2d_patches.len()
    );

    if conv2d_patches.is_empty() {
        println!("‚ö†Ô∏è  No conv2d patches found, skipping relevance test");
        return;
    }

    // Use first conv2d patch as query source
    let query_idx = conv2d_patches[0].0;
    let embeddings = sphere_ebm.embeddings.clone();
    let embed_dim = sphere_ebm.embedding_dim();
    let query: Tensor<WgpuBackend, 1> = embeddings
        .slice([query_idx..query_idx + 1, 0..embed_dim])
        .reshape([embed_dim]);

    println!("Using patch {} as query (contains conv2d)", query_idx);

    // Run navigation
    let result = navigator.navigate_multi_cone(query, 50.0, 10, RngKey::new(456), &device);

    println!("\nTop results for conv2d query:");
    let mut conv2d_hits = 0;
    for (i, idx) in result.target_indices.iter().take(10).enumerate() {
        let patch = &patch_bytes[*idx];
        let text = String::from_utf8_lossy(patch);
        let has_conv2d =
            text.to_lowercase().contains("conv2d") || text.to_lowercase().contains("convolution");
        if has_conv2d {
            conv2d_hits += 1;
        }
        let marker = if has_conv2d { "‚úì" } else { " " };
        let preview = text.chars().take(60).collect::<String>();
        println!("  {}. {} [{}]: {:?}...", i + 1, marker, idx, preview);
    }

    println!(
        "\nRelevance: {}/{} top results contain conv2d/convolution",
        conv2d_hits,
        result.target_indices.len().min(10)
    );

    // At least some results should be relevant (but not required for test to pass)
    if conv2d_hits == 0 && !result.is_empty() {
        println!("‚ö†Ô∏è  Warning: No conv2d matches in top results");
    }
}

#[test]
fn test_gauntlet_throughput_metrics() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let path = Path::new(get_gauntlet_path());

    println!("=== Gauntlet Test: Throughput Metrics ===");

    // Measure load time
    let load_start = Instant::now();
    let (sphere_ebm, patch_bytes) =
        load_blt_safetensors(path, SphereConfig::from(ScaleProfile::Dev), &device)
            .expect("Failed to load");
    let load_time = load_start.elapsed();

    let n_patches = sphere_ebm.n_points();
    let total_bytes: usize = patch_bytes.iter().map(|p| p.len()).sum();
    let load_throughput = total_bytes as f64 / load_time.as_secs_f64() / 1024.0;

    println!(
        "Load: {} bytes in {:?} ({:.1} KB/s)",
        total_bytes, load_time, load_throughput
    );

    // ROOTS has O(n¬≤) pairwise similarity - skip ROOTS/navigation benchmarks if too large
    if n_patches > MAX_PATCHES_FOR_ROOTS {
        println!(
            "‚ö†Ô∏è  Skipping ROOTS/navigation benchmarks: {} patches > {} limit (O(n¬≤) too slow)",
            n_patches, MAX_PATCHES_FOR_ROOTS
        );
        println!("\n=== Metrics Summary ===");
        println!("  Load throughput: {:.1} KB/s", load_throughput);
        println!("  ROOTS: skipped (corpus too large)");
        println!("  Navigation: skipped (corpus too large)");
        return;
    }

    // Measure ROOTS build time
    let roots_start = Instant::now();
    let roots_config = RootsConfig::dev().with_partitions(16);
    let navigator = MultiConeNavigator::from_sphere_ebm_with_bytes(
        &sphere_ebm,
        &patch_bytes,
        roots_config,
        BudgetConfig::dev(),
        RngKey::new(42),
        &device,
    );
    let roots_time = roots_start.elapsed();

    let stats = navigator.roots.stats();
    let embed_dim = sphere_ebm.embedding_dim();
    let raw_embed_size = sphere_ebm.n_points() * embed_dim * 4;
    let compression = raw_embed_size as f64 / stats.memory_bytes as f64;

    println!(
        "ROOTS: {} partitions in {:?} ({:.1}:1 compression)",
        stats.n_partitions, roots_time, compression
    );

    // Measure navigation time (average over multiple queries)
    let mut navigator = navigator;
    let n_queries = 10;
    let embeddings = sphere_ebm.embeddings.clone();
    let n_points = sphere_ebm.n_points();

    let nav_start = Instant::now();
    let embed_dim = sphere_ebm.embedding_dim();
    for i in 0..n_queries {
        let query_idx = i % n_points;
        let query: Tensor<WgpuBackend, 1> = embeddings
            .clone()
            .slice([query_idx..query_idx + 1, 0..embed_dim])
            .reshape([embed_dim]);
        let _ = navigator.navigate_multi_cone(query, 50.0, 5, RngKey::new(i as u64), &device);
    }
    let nav_time = nav_start.elapsed();
    let avg_nav_time = nav_time.as_secs_f64() / n_queries as f64;

    println!(
        "Navigation: {} queries in {:?} ({:.1}ms avg)",
        n_queries,
        nav_time,
        avg_nav_time * 1000.0
    );

    // Summary
    println!("\n=== Metrics Summary ===");
    println!("  Load throughput: {:.1} KB/s", load_throughput);
    println!("  ROOTS compression: {:.1}:1", compression);
    println!("  Avg navigation: {:.1}ms", avg_nav_time * 1000.0);

    // Assertions (relaxed for small test data)
    // Note: Small datasets don't compress well; at scale ratios are much better
    assert!(load_throughput > 10.0, "Load should be > 10 KB/s");
    assert!(compression > 1.0, "Index should be smaller than raw");
    assert!(
        avg_nav_time < 10.0,
        "Navigation should complete in reasonable time"
    );
}

// ============================================================================
// Spring / Hypergraph Tests (requires "hypergraph" feature)
// ============================================================================

/// Test loading hypergraph sidecar and inspecting its structure.
#[cfg(feature = "hypergraph")]
#[test]
fn test_gauntlet_hypergraph_loading() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let safetensors_path = Path::new(get_gauntlet_path());

    // Derive hypergraph path
    let hg_path = safetensors_path.with_extension("hypergraph.db");

    println!("=== Gauntlet Test: Hypergraph Loading ===");
    println!("SafeTensors: {}", safetensors_path.display());
    println!("Hypergraph:  {}", hg_path.display());

    if !hg_path.exists() {
        println!("‚ö†Ô∏è  No hypergraph sidecar found at {:?}", hg_path);
        println!("   Run blt-burn with --export-json to generate it");
        return;
    }

    // Get hypergraph stats
    match hypergraph_stats(&hg_path) {
        Ok(stats) => {
            println!("\nHypergraph Stats:");
            println!("{}", stats);

            assert!(stats.total_nodes > 0, "Should have nodes");
            assert!(stats.total_edges > 0, "Should have edges");

            // "next" edges are the key ones for spring physics
            println!(
                "\nEdge breakdown: {} next, {} contains, {} other",
                stats.next_edges, stats.contains_edges, stats.other_edges
            );
        }
        Err(e) => {
            println!("Failed to get hypergraph stats: {}", e);
        }
    }

    // Load with hypergraph
    println!("\nLoading SafeTensors + Hypergraph...");
    let start = std::time::Instant::now();

    let result = load_blt_with_hypergraph(
        safetensors_path,
        SphereConfig::from(ScaleProfile::Dev),
        HypergraphLoadConfig::for_code(),
        &device,
    );

    match result {
        Ok((sphere_ebm, patch_bytes, hg_ebm_opt)) => {
            let elapsed = start.elapsed();
            println!("‚úÖ Loaded in {:?}", elapsed);
            println!("   Patches: {}", sphere_ebm.n_points());
            println!("   Bytes: {} vectors", patch_bytes.len());

            if let Some(hg_ebm) = hg_ebm_opt {
                println!("   Hypergraph: ‚úì LOADED (springs enabled!)");

                // CRITICAL: Verify coherence was extracted from sidecar
                // This tests the fix in hypergraph_loader.rs that extracts
                // prominence from metadata.extra and populates node_weights
                if hg_ebm.coherence.is_some() {
                    println!("   Coherence:  ‚úì EXTRACTED from sidecar (prominence ‚Üí node_weights)");
                } else {
                    println!("   Coherence:  ‚úó NOT extracted (node_weights empty)");
                    println!(
                        "   ‚ö†Ô∏è  This may indicate blt-burn sidecar doesn't contain prominence"
                    );
                    println!("   ‚ö†Ô∏è  Re-run ingestion with unified pipeline to fix");
                }
            } else {
                println!("   Hypergraph: ‚úó not loaded");
            }
        }
        Err(e) => {
            panic!("Failed to load: {}", e);
        }
    }
}

/// Test navigation with spring physics enabled via hypergraph.
#[cfg(feature = "hypergraph")]
#[test]
fn test_gauntlet_spring_navigation() {
    if skip_if_no_gauntlet_data() {
        return;
    }

    let device = init_gpu_device();
    let safetensors_path = Path::new(get_gauntlet_path());
    let hg_path = safetensors_path.with_extension("hypergraph.db");

    println!("=== Gauntlet Test: Spring-Enabled Navigation ===");

    if !hg_path.exists() {
        println!("‚ö†Ô∏è  Skipping spring test: no hypergraph sidecar");
        return;
    }

    // Load everything
    let (sphere_ebm, patch_bytes, hg_ebm_opt) = load_blt_with_hypergraph(
        safetensors_path,
        SphereConfig::from(ScaleProfile::Dev),
        HypergraphLoadConfig::for_code()
            .with_spring_constant(0.5)
            .with_coherence_weight(0.3),
        &device,
    )
    .expect("Failed to load");

    println!("Loaded {} patches", sphere_ebm.n_points());

    // Create navigator WITH springs
    let mut navigator = NavigatorEBM::from_sphere_ebm(sphere_ebm.clone());

    if let Some(hg_ebm) = hg_ebm_opt {
        println!("üîó Springs ENABLED via hypergraph");
        navigator = navigator.with_hypergraph(hg_ebm);
    } else {
        println!("‚ö†Ô∏è  No hypergraph loaded, springs disabled");
    }

    // Set weights to include graph energy
    navigator = navigator.with_weights(
        NavigationWeights::default()
            .with_semantic(1.0)
            .with_radial(0.3)
            .with_graph(0.5) // Enable graph/spring energy term
            .with_entropy(0.2),
    );

    // Create query
    let embeddings = sphere_ebm.embeddings.clone();
    let embed_dim = sphere_ebm.embedding_dim();
    let query: Tensor<WgpuBackend, 1> = embeddings.slice([0..1, 0..embed_dim]).reshape([embed_dim]);

    // Run navigation
    println!("\nRunning spring-enabled navigation...");
    let start = std::time::Instant::now();
    let result = navigator.navigate(query, 50.0, RngKey::new(42), 10, &device);
    let elapsed = start.elapsed();

    println!("‚úÖ Navigation completed in {:?}", elapsed);
    println!("   Targets: {}", result.target_indices.len());
    println!("   Total energy: {:.4}", result.total_energy);

    // Show results
    if !result.target_indices.is_empty() {
        println!("\nTop results:");
        for (i, (idx, energy)) in result
            .target_indices
            .iter()
            .zip(result.target_energies.iter())
            .take(5)
            .enumerate()
        {
            let patch = &patch_bytes[*idx];
            let preview = String::from_utf8_lossy(&patch[..patch.len().min(50)]);
            println!(
                "  {}. [{}] energy={:.4}: {:?}...",
                i + 1,
                idx,
                energy,
                preview
            );
        }
    }

    // The key test: with springs, sequential patches should cluster
    // Check if adjacent patch indices appear near each other in results
    let result_indices: std::collections::HashSet<usize> =
        result.target_indices.iter().copied().collect();

    let mut adjacent_pairs = 0;
    for &idx in &result.target_indices {
        if result_indices.contains(&(idx + 1)) || (idx > 0 && result_indices.contains(&(idx - 1))) {
            adjacent_pairs += 1;
        }
    }

    println!(
        "\nSpring effect: {}/{} results have adjacent patches in results",
        adjacent_pairs,
        result.target_indices.len()
    );

    // Note: We don't assert on adjacency since it depends on the specific data
    // The test primarily verifies the spring machinery runs without errors
}
